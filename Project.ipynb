{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncovering the Liars in the Political Scene of America\n",
    "\n",
    "This notebook will present a data analysis task on the \"Liar, Liar Pants on Fire\" dataset, available for download [there](https://www.cs.ucsb.edu/Ëœwilliam/data/liar_dataset.zip), in the framework of a project for the class \"Applied Data Analysis\" at EPFL. More precisely, it will present the work done for milestone 2 and a plan up until milestone 3.\n",
    "\n",
    "### Table of contents:\n",
    "* [Loading phase](#loading-phase)\n",
    "* [Cleaning phase](#cleaning-phase)\n",
    "* [Loading additional files](#loading-additional-files)\n",
    "* [Data analysis](#data-analysis)\n",
    "    * [Bar chart](#bar-chart)\n",
    "    * [Relative importance of each feature](#relative-importance)\n",
    "    * [Map visualization](#map-visualization)\n",
    "* [Plan for milestone 3](#plan-ms-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import folium\n",
    "import ipywidgets as ipw\n",
    "import html\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading phase <a class=\"anchor\" id=\"loading-phase\"></a>\n",
    "\n",
    "The liar dataset is composed of three different datasets, namely `test.tsv`, `train.tsv` and `valid.tsv`, which we first load into Pandas dataframes with corresponding names. All of these datasets have exactly the same schema. The purpose of having three different datasets is to do machine learning, which is not the goal of this project. In consequence, we will combine all three dataframes `test`, `train` and `valid` into a single one, `liar`.\n",
    "\n",
    "The schema of these datasets is outlined below. There is a total of 14 columns and each row refers to a statement.\n",
    "\n",
    "> Column 1: ID of the statement ([ID].json).\n",
    "> \n",
    "> Column 2: Label.\n",
    "> \n",
    "> Column 3: Statement.\n",
    "> \n",
    "> Column 4: Subject(s).\n",
    "> \n",
    "> Column 5: Speaker.\n",
    "> \n",
    "> Column 6: Speaker's job title.\n",
    "> \n",
    "> Column 7: State info.\n",
    "> \n",
    "> Column 8: Party affiliation.\n",
    "> \n",
    "> Column 9: Barely true counts.\n",
    "> \n",
    "> Column 10: False counts.\n",
    "> \n",
    "> Column 11: Half true counts.\n",
    "> \n",
    "> Column 12: Mostly true counts.\n",
    "> \n",
    "> Column 13: Pants on fire counts.\n",
    "> \n",
    "> Column 14: Context (venue / location of the speech or statement).\n",
    "\n",
    "In order to reduce the size of the dataframe, we could drop the columns which we do not need. Note that this is not required because the dataset is rather small, but could be done simply for the sake of convenience. However, we will probably need all columns, so no action will be taken there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "SCHEMA = ['statement_id', 'label', 'statement', 'subject', \n",
    "          'speaker', 'profession', 'state', 'party', \n",
    "          'barely_true', 'false', 'half_true', \n",
    "          'mostly_true', 'pants_on_fire', 'context']\n",
    "\n",
    "# Load the datasets into pandas dataframes\n",
    "test = pd.read_csv(DATA_DIR + 'test.tsv', delimiter='\\t', header=None, names=SCHEMA, index_col=False)\n",
    "train = pd.read_csv(DATA_DIR + 'train.tsv', delimiter='\\t', header=None, names=SCHEMA, index_col=False)\n",
    "valid = pd.read_csv(DATA_DIR + 'valid.tsv', delimiter='\\t', header=None, names=SCHEMA, index_col=False)\n",
    "\n",
    "\n",
    "# Combine the three dataframes into one\n",
    "liar = pd.concat([train, test, valid], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "liar.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the three datasets are different (which they should based on their purpose):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar['statement_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning phase <a class=\"anchor\" id=\"cleaning-phase\"></a>\n",
    "\n",
    "This dataset id very clean by nature because it is not just a collection of data. Indeed, it was intended for use by other data scientists as a benchmark dataset and it will therefore not need many data cleaning operations. We can start by checking if every row is complete or if there are any NaNs or missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that most of the columns, more precisely column 4 to 15, do have missing values. However, we will not drop every row where there are NaNs because the other fields in these rows might still be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now investigate how the data is formatted to check for potential inconcistencies. We can start with the column concerning the state where the statement was made, because we can have a fairly good idea of what it should contain (i.e. names of locations like states or countries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar.state.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are inconcistencies in the use of upper case (e.g. `Ohio` and `ohio`) and problems with trailing spaces (e.g. `Georgia` and `Georgia  `). We can take care of these issues by making everything lower case and removing leading and trailing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    '''\n",
    "    Replaces upper case letters by lower case ones and removes leading and trailing spaces.\n",
    "    :param s: str\n",
    "    :return: s\n",
    "    '''\n",
    "    if isinstance(s, str):\n",
    "        s = s.lower()\\\n",
    "             .strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows before dropping \"all Nans\" rows\n",
    "print('Number of unique entities in \"state\" before applying \"clean\": %s'%liar.state.unique().shape[0])\n",
    "\n",
    "liar['state'] = liar['state'].apply(clean)\n",
    "\n",
    "# Number of rows before dropping \"all Nans\" rows\n",
    "print('Number of unique entities in \"state\" after  applying \"clean\": %s'%liar.state.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are still other issues with the data, but we will not take care of them. For example, there are spelling mistakes (e.g. `virgiia` instead of `virginia`). We will ignore them because we can assume that these spelling mistakes will concern only one statement, which means it is not worth taking the time to take care of that kind of issues.\n",
    "\n",
    "In order to clean the other columns in liar, we will use the same function as above, `clean`. We can indeed assume that they might suffer from the same inconsistencies. It is important to point out that we will not touch to the column `statement` because we want to keep all statements as they were originally written. If we were to modify them, then all conclusions from the analysis of the statements themselves would loose credibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in liar.columns:\n",
    "    if not (col_name == 'statement'):\n",
    "        liar[col_name] = liar[col_name].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a final look at our cleaned `liar` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "liar.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading additional files <a class=\"anchor\" id=\"loading-additional-files\"></a>\n",
    "\n",
    "In order to create maps using folium displaying content from the `liar` dataframe, a new column with the abbreviations of the states will be added. The pairs state name/abbreviations are all contained in the file `states_abbreviation.csv`, which can be downloaded [here](http://www.fonz.net/blog/archives/2008/04/06/csv-of-states-and-state-abbreviations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file 'states_abbreviation.csv' into a dataframe\n",
    "states_abbreviation = pd.read_csv(DATA_DIR + 'states_abbreviation.csv')\n",
    "\n",
    "# Make sure we have the same format than in liar\n",
    "states_abbreviation['State'] = states_abbreviation['State'].apply(clean)\n",
    "\n",
    "i = 0\n",
    "for state_name in liar['state']:\n",
    "    temp = states_abbreviation.Abbreviation[state_name==states_abbreviation.State].values\n",
    "    if temp.size != 0:\n",
    "        liar.at[i, 'state_abbreviation'] = temp[0]\n",
    "    else:\n",
    "        liar.at[i, 'state_abbreviation'] = np.nan\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "liar.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will also load two additional datasets, `pop-urban-pct-historical.xls` and `federalelections2016.xlsx`, which will have the purpose of highlighting potential explanations for the distribution of fake news across America. They are available for download respectively [there](https://www.icip.iastate.edu/tables/population/urban-pct-states) and [there](https://transition.fec.gov/pubrec/electionresults.shtml). For the same reason as earlier, we will add a column with the abbreviations of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pop-urban-pct-historical.xls\n",
    "states_urban = pd.read_excel(DATA_DIR + 'pop-urban-pct-historical.xls', sheet_name='States',\n",
    "                             header=5, usecols='B:G,I:M,O:P')\n",
    "states_urban = states_urban[1:52]\n",
    "\n",
    "# Make sure we have the same format than in states_abbreviation\n",
    "states_urban['Area Name'] = states_urban['Area Name'].apply(clean)\n",
    "\n",
    "# Add the states abbreviations\n",
    "i = 1\n",
    "for state_name in states_urban['Area Name']:\n",
    "    temp = states_abbreviation.Abbreviation[state_name==states_abbreviation.State].values\n",
    "    if temp.size != 0:\n",
    "        states_urban.at[i, 'state_abbreviation'] = temp[0]\n",
    "    else:\n",
    "        states_urban.at[i, 'state_abbreviation'] = np.nan\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "states_urban.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load federalelections2016.xlsx\n",
    "states_election = pd.read_excel(DATA_DIR + 'federalelections2016.xlsx', sheet_name=15,\n",
    "                                header=6, usecols='A:B,E,H,K,N,Q')\n",
    "states_election = states_election[1:52]\n",
    "states_election = states_election.rename(columns={\"CLINTON\": 2016, \"OBAMA\": 2012,\n",
    "                                                  \"OBAMA.1\": 2008, \"KERRY\": 2004,\n",
    "                                                  \"GORE\": 2000, \"CLINTON.1\": 1996})\n",
    "\n",
    "# Make sure we have the same format than in states_abbreviation\n",
    "states_election['STATE'] = states_election['STATE'].apply(clean)\n",
    "\n",
    "# Add the states abbreviations\n",
    "i = 1\n",
    "for state_name in states_election['STATE']:\n",
    "    temp = states_abbreviation.Abbreviation[state_name==states_abbreviation.State].values\n",
    "    if temp.size != 0:\n",
    "        states_election.at[i, 'state_abbreviation'] = temp[0]\n",
    "    else:\n",
    "        states_election.at[i, 'state_abbreviation'] = np.nan\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "states_election.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis <a class=\"anchor\" id=\"data-analysis\"></a>\n",
    "\n",
    "After the initialization with the loading and cleaning phases, we can start the data anlysis _per se_. First, we will create a simple bar chart showing the number of statements made in each states. Then, we will create a visualization tool to understand which feature plays an important role. Finally, we will look at the geographical distribution of fake news and try to correlate it with additional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar chart <a class=\"anchor\" id=\"bar-chart\"></a>\n",
    "\n",
    "The goal here is to get a first idea of how many statements were made in each states. For that, we will make a stacked bar chart showing the number of statements for every label and for each states. Note that we will consider here only american states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe with the number of statements for each state and for each label\n",
    "state_count = liar.dropna(subset=['state_abbreviation'])\\\n",
    "                  .groupby(['state','label'])\\\n",
    "                  .count()\\\n",
    "                  .reset_index()\\\n",
    "                  .filter(['state', 'label', 'statement_id'])\\\n",
    "                  .rename(columns={'statement_id':'#'})\n",
    "\n",
    "# Get a dataframe for each label with the number of statements for each state\n",
    "df_total       = liar.dropna(subset=['state_abbreviation']).groupby('state').count().filter(['statement_id']).rename(columns={'statement_id':'#_total'})\n",
    "df_pants_fire  = state_count[state_count['label']=='pants-fire'] .drop('label', axis=1).set_index('state')\n",
    "df_false       = state_count[state_count['label']=='false']      .drop('label', axis=1).set_index('state')\n",
    "df_barely_true = state_count[state_count['label']=='barely-true'].drop('label', axis=1).set_index('state')\n",
    "df_half_true   = state_count[state_count['label']=='half-true']  .drop('label', axis=1).set_index('state')\n",
    "df_mostly_true = state_count[state_count['label']=='mostly-true'].drop('label', axis=1).set_index('state')\n",
    "df_true        = state_count[state_count['label']=='true']       .drop('label', axis=1).set_index('state')\n",
    "\n",
    "# Join all dataframes together\n",
    "temp1 = df_total.join(df_pants_fire, how='outer')\\\n",
    "                .join(df_false, how='outer', rsuffix=' false')\\\n",
    "                .join(df_barely_true, how='outer', rsuffix=' barely true')\\\n",
    "                .join(df_half_true, how='outer', rsuffix=' half true')\\\n",
    "                .join(df_mostly_true, how='outer', rsuffix=' mostly true')\\\n",
    "                .join(df_true, how='outer', rsuffix=' true')\\\n",
    "                .rename(columns={'#':'# pants on fire'})\\\n",
    "                .fillna(0)\\\n",
    "                .sort_values(by=['#_total'], ascending=False)\\\n",
    "                .drop('#_total', axis=1)\n",
    "\n",
    "# Create a stacked plot\n",
    "temp1.plot.bar(stacked=True, title='Number of statements made in each state', figsize=(20,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of statements is varying greatly between states. It is uncertain whether this distribution is representative of the real distribution or results from the way the statements are collected. This might make the comparison between states uneasy if not totally meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative importance of each feature <a class=\"anchor\" id=\"relative-importance\"></a>\n",
    "\n",
    "In order to investigate and visualize the data contained in `liar`, we can create a widget which takes as inputs each of the different metadata associated to a statement and returns as output the number of statements which have this set of metadata. This will help understanding how important every features (like profession, subject, etc.) of the statements are and also help finding interesting patterns.\n",
    "\n",
    "Before we can do that, we have to define a function which will be able to retrieve only the most relevant and occurent values for each feature. For example, for the parties, we will care about the Republicans and the Democrats but less about other minor entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_x(col_name, x, sorted=False):\n",
    "    '''\n",
    "    Returns a sorted (if needed) list of the top x entities with the most occurencies in the column specified by col_name.\n",
    "    :param col_name: str, x: int, sorted: bool\n",
    "    :return: top_ten\n",
    "    '''\n",
    "    # Get the top x\n",
    "    top_x = liar[['statement_id', col_name]].groupby(col_name)\\\n",
    "                                            .count()\\\n",
    "                                            .sort_values('statement_id', ascending=False)\\\n",
    "                                            .head(x)\\\n",
    "                                            .index\\\n",
    "                                            .values\\\n",
    "                                            .astype('str')\n",
    "    \n",
    "    # Create an array of tupple where the first element is like the second but its dashes are replaced by spaces and each word is capitalized\n",
    "    top_x = [(s.replace('-', ' ').title(), s) for s in top_x]\n",
    "    \n",
    "    # If sorted is True, sort the list by alphabetical order\n",
    "    if sorted:\n",
    "        top_x.sort(key=lambda x: x[0])\n",
    "    return top_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need in order to build our widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of statements\n",
    "nb_tot = liar.shape[0]\n",
    "\n",
    "# Replace all NaNs by 'NA' to avoid categorizing statements as excluded when they should be included\n",
    "liarNA = liar.fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lable_proportion(toggle_dots, datapoints_per_dot, label, subject, speaker, profession, state, party, context):\n",
    "    '''\n",
    "    Print the proportion of statements which have the properties specified by the inputs.\n",
    "    :param toggle_dots: bool, datapoints_per_dot: int, label-context: str\n",
    "    :return: None\n",
    "    '''\n",
    "    # In case \"All <input>\" is required, replace the corresponding input by the column it corresponds to in liarNA\n",
    "    # so that the filter wrt this input contains only \"True\" values. \n",
    "    if subject == 'all_subjects':\n",
    "        subject = liarNA.subject\n",
    "    if speaker == 'all_speakers':\n",
    "        speaker = liarNA.speaker\n",
    "    if profession == 'all_professions':\n",
    "        profession = liarNA.profession\n",
    "    if state == 'all_states':\n",
    "        state = liarNA.state\n",
    "    if party == 'all_parties':\n",
    "        party = liarNA.party\n",
    "    if context == 'all_contexts':\n",
    "        context = liarNA.context\n",
    "    \n",
    "    # Compute the filters to keep only statements having the properties specified by the inputs\n",
    "    filter_all_but_labels = (liarNA.subject==subject)       & (liarNA.speaker==speaker) & \\\n",
    "                            (liarNA.profession==profession) & (liarNA.state==state)     & \\\n",
    "                            (liarNA.party==party)           & (liarNA.context==context)\n",
    "    \n",
    "    filter_pants_on_fire = (liarNA.label=='pants-fire')  & filter_all_but_labels\n",
    "    filter_false         = (liarNA.label=='false')       & filter_all_but_labels\n",
    "    filter_barely_true   = (liarNA.label=='barely-true') & filter_all_but_labels\n",
    "    filter_half_true     = (liarNA.label=='half-true')   & filter_all_but_labels\n",
    "    filter_mostly_true   = (liarNA.label=='mostly-true') & filter_all_but_labels\n",
    "    filter_true          = (liarNA.label=='true')        & filter_all_but_labels\n",
    "    \n",
    "    # Apply the filters and count the number of remaining statements for each label.\n",
    "    if label == 'all_labels':\n",
    "        nb_pants_on_fire = round(liarNA[filter_pants_on_fire].shape[0] / datapoints_per_dot)\n",
    "        nb_false         = round(liarNA[filter_false]        .shape[0] / datapoints_per_dot)\n",
    "        nb_barely_true   = round(liarNA[filter_barely_true]  .shape[0] / datapoints_per_dot)\n",
    "        nb_half_true     = round(liarNA[filter_half_true]    .shape[0] / datapoints_per_dot)\n",
    "        nb_mostly_true   = round(liarNA[filter_mostly_true]  .shape[0] / datapoints_per_dot)\n",
    "        nb_true          = round(liarNA[filter_true]         .shape[0] / datapoints_per_dot)\n",
    "    else:\n",
    "        nb_pants_on_fire = round(liarNA[filter_pants_on_fire & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_false         = round(liarNA[filter_false         & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_barely_true   = round(liarNA[filter_barely_true   & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_half_true     = round(liarNA[filter_half_true     & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_mostly_true   = round(liarNA[filter_mostly_true   & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_true          = round(liarNA[filter_true          & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "    \n",
    "    # Count the number of statements which does not have the properties specified by the inputs\n",
    "    nb_others = round(nb_tot/datapoints_per_dot - (nb_pants_on_fire + nb_false + nb_barely_true +\n",
    "                                                   nb_half_true + nb_mostly_true + nb_true))\n",
    "    \n",
    "    # Print the legend\n",
    "    print('\\033[1mThere is a total of %s statements, out of which %s satisfy your requirements.\\033[0m'%(nb_tot, nb_tot-nb_others*datapoints_per_dot))\n",
    "    print('\\n\\033[41m    \\033[0m Pants on fire    ' +\n",
    "            '\\033[42m    \\033[0m False    '         +\n",
    "            '\\033[43m    \\033[0m Barely true    '   +\n",
    "            '\\033[44m    \\033[0m Half true    '     +\n",
    "            '\\033[45m    \\033[0m Mostly true    '   +\n",
    "            '\\033[46m    \\033[0m True    '          +\n",
    "            '\\033[40m    \\033[0m Excluded\\n')\n",
    "    \n",
    "    # Print colored areas proportional to the number of statements satisfying the requirements\n",
    "    if toggle_dots:# With dots\n",
    "        print('\\033[41m' + html.unescape(nb_pants_on_fire*'&#x25CF') + '\\033[0m' +\n",
    "              '\\033[42m' + html.unescape(nb_false*'&#x25CF')         + '\\033[0m' +\n",
    "              '\\033[43m' + html.unescape(nb_barely_true*'&#x25CF')   + '\\033[0m' +\n",
    "              '\\033[44m' + html.unescape(nb_half_true*'&#x25CF')     + '\\033[0m' +\n",
    "              '\\033[45m' + html.unescape(nb_mostly_true*'&#x25CF')   + '\\033[0m' +\n",
    "              '\\033[46m' + html.unescape(nb_true*'&#x25CF')          + '\\033[0m' +\n",
    "              '\\033[40m' + html.unescape(nb_others*'&#x25CF')        + '\\033[0m')\n",
    "    else:# Without dots\n",
    "        print('\\033[41m' + nb_pants_on_fire*' ' + '\\033[0m' +\n",
    "              '\\033[42m' + nb_false*' '         + '\\033[0m' +\n",
    "              '\\033[43m' + nb_barely_true*' '   + '\\033[0m' +\n",
    "              '\\033[44m' + nb_half_true*' '     + '\\033[0m' +\n",
    "              '\\033[45m' + nb_mostly_true*' '   + '\\033[0m' +\n",
    "              '\\033[46m' + nb_true*' '          + '\\033[0m' +\n",
    "              '\\033[40m' + nb_others*' '        + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a widget for the function above (lable_proportion)\n",
    "ipw.interact(lable_proportion,\n",
    "             toggle_dots = ipw.widgets.ToggleButton(value=False, description='Toggle dots', button_style='success', tooltip='Show/hide the dots representing each statements'),\n",
    "             datapoints_per_dot = ipw.widgets.IntSlider(value=1., min=1., max=10, description='#statements per dot', style={'description_width': 'initial'}),\n",
    "             label      = [('All labels', 'all_labels'), ('Pants on fire', 'pants-fire'),\n",
    "                           ('False', 'false'), ('Barely true', 'barely-true'),\n",
    "                           ('Half true', 'half-true'), ('Mostly true', 'mostly-true'),\n",
    "                           ('True', 'true')],\n",
    "             subject    = [('All subjects',    'all_subjects'   )] + find_top_x('subject',    10, sorted=True),\n",
    "             speaker    = [('All speakers',    'all_speakers'   )] + find_top_x('speaker',     5, sorted=False),\n",
    "             profession = [('All professions', 'all_professions')] + find_top_x('profession', 10, sorted=True),\n",
    "             state      = [('All states',      'all_states'     )] + find_top_x('state',      99, sorted=True),\n",
    "             party      = [('All parties',     'all_parties'    )] + find_top_x('party',       5, sorted=False),\n",
    "             context    = [('All contexts',    'all_contexts'   )] + find_top_x('context',    10, sorted=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map visualization <a class=\"anchor\" id=\"map-visualization\"></a>\n",
    "\n",
    "As we have information concerning where each statement was made, we can visualize their distribution geographically on a map. This will allow us to determine whether there are states where politicians tend to lie more. These states (if any) would be more vulnerable to potentially harmful consequences of fake news and thus identifying them is an important task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geo json file to display the states\n",
    "states_geo_json = json.load(open(DATA_DIR + r'us-states.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_map_label(label):\n",
    "    '''\n",
    "    Plots a map of the USA indicating the number of statements per states for a selected label.\n",
    "    :param label: str\n",
    "    :return: None\n",
    "    '''\n",
    "    # Filter out all statements with labels different from label\n",
    "    filter_label = liar.label == label\n",
    "    \n",
    "    # Count the number of remaining statements per state\n",
    "    state_count = liar[filter_label].groupby('state_abbreviation')\\\n",
    "                                    .count()\\\n",
    "                                    .reset_index()\\\n",
    "                                    .filter(['state_abbreviation', 'statement_id'])\n",
    "    \n",
    "    # Add the states where no statements were made (so that they do not appear with the darkest color on the map)\n",
    "    for state in states_abbreviation['Abbreviation']:\n",
    "        if not liar[filter_label]['state_abbreviation'].str.contains(state, regex=False).any():\n",
    "            state_count = state_count.append(pd.DataFrame([(state, 0)],\n",
    "                                             columns=['state_abbreviation', 'statement_id']))\n",
    "    \n",
    "    # Create a map centered around the USA\n",
    "    m_usa = folium.Map([43,-100], tiles='cartodbpositron', zoom_start=4)\n",
    "    \n",
    "    # Add a filter to the map indicating the number of statements in each state\n",
    "    m_usa.choropleth(geo_data=states_geo_json, data=state_count,\n",
    "                    columns=['state_abbreviation', 'statement_id'],\n",
    "                    key_on='feature.id',\n",
    "                    fill_color='YlOrRd', fill_opacity=0.7, line_opacity=1,\n",
    "                    legend_name=('Number of %s statements in each state'%label),\n",
    "                    highlight=True)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m_usa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to just visualizing the number of statements per state, we can also compare the results with the data contained in the two additional datasets which we loaded earlier, `pop-urban-pct-historical.xls` and `federalelections2016.xlsx`. For that, we will display another map right below the previous one in order to allow a direct comparison between the two maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_map_comparison(select_data):\n",
    "    '''\n",
    "    Plots a map of the USA with either information on urbanity or elections outcome.\n",
    "    :param select_data: str\n",
    "    :return: None\n",
    "    '''\n",
    "    # Select the right dataset\n",
    "    if select_data[0:-4] == 'urb_':\n",
    "        data = states_urban.filter(['state_abbreviation', int(select_data[-4:])])\n",
    "        legend = 'Percentage of the population living in urban areas'\n",
    "    if select_data[0:-4] == 'elec_':\n",
    "        data = states_election.filter(['state_abbreviation', int(select_data[-4:])])\n",
    "        legend = 'Percentage of vote for the democrats'\n",
    "    \n",
    "    # Create a map centered around the USA\n",
    "    m_usa = folium.Map([43,-100], tiles='cartodbpositron', zoom_start=4)\n",
    "    \n",
    "    # Add a filter to the map indicating the required data\n",
    "    m_usa.choropleth(geo_data=states_geo_json, data=data,\n",
    "                    columns=['state_abbreviation', int(select_data[-4:])],\n",
    "                    key_on='feature.id',\n",
    "                    fill_color='RdBu', fill_opacity=0.7, line_opacity=1,\n",
    "                    legend_name=legend,\n",
    "                    highlight=True)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a widget for the function plot_map_label\n",
    "ipw.interact(plot_map_label, label=[('Pants on fire', 'pants-fire'), ('False', 'false'), ('Barely true', 'barely-true'),\n",
    "                           ('Half true', 'half-true'), ('Mostly true', 'mostly-true'), ('True', 'true')]);\n",
    "\n",
    "# Create a widget for the function plot_map_comparison\n",
    "ipw.interact(plot_map_comparison, select_data=[('Urban percentage 2010', 'urb_2010'), ('Urban percentage 2000', 'urb_2010'),\n",
    "                                               ('Election 2016', 'elec_2016'), ('Election 2012', 'elec_2012'),\n",
    "                                               ('Election 2008', 'elec_2008'), ('Election 2004', 'elec_2004'),\n",
    "                                               ('Election 2000', 'elec_2000'), ('Election 2096', 'elec_2096')]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan for milestone 3 <a class=\"anchor\" id=\"plan-ms-3\"></a>\n",
    "\n",
    "The dataset that we have chosen is quite rich with information, given the statement texts (the corpus), and we plan to use this to go beyond the labels provided by the dataset. We will use NLTK (Natural Language Toolkit) and RAKE (Rapid Automatic Keyword Extraction) to analyze the corpus; find key phrases, discover buzzwords, and so on.\n",
    "\n",
    "We also want to explore and visualize the data by making a word cloud (using WordCloud) in Python.\n",
    "\n",
    "Furthermore, we would like to explore the relationships between the speakers; the inter-person lies - who lies about who? In order to visualize this, we will use NetworkX to create a directed graph. Perhaps we will discover a bipartite graph showing that republicans only lie about democrats, and vice versa?\n",
    "\n",
    "Additionally, by using a Venn diagram, we would like to see if there are topics that only republicans, or only democrats, lie about, and which topics that are subjected to lies by both parties.\n",
    "\n",
    "Finally, we would like to show the topic most prominent to lies in each state, by using Folium to create a map with interactive markers. This is because we want to see which topic that is most often the subject of fake news in each state, as some topics are likely to be of higher importance in some states. For example, discussion of immigration laws, or gun control, might be more present in states with a lot of immigration and guns. We would like to examine these connections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
