{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncovering the Liars in the Political Scene of America\n",
    "\n",
    "This notebook will present a data analysis task on the \"Liar, Liar Pants on Fire\" dataset, available for download [here](https://www.cs.ucsb.edu/Ëœwilliam/data/liar_dataset.zip), in the framework of a project for the class \"Applied Data Analysis\" at EPFL. More precisely, it will present the work done for milestone 2 and a plan up until milestone 3.\n",
    "\n",
    "### Table of contents:\n",
    "* [Milestone 2](#milestone-2)\n",
    "    * [Loading phase](#loading-phase)\n",
    "    * [Cleaning phase](#cleaning-phase)\n",
    "    * [Loading additional files](#loading-additional-files)\n",
    "    * [Data analysis](#data-analysis)\n",
    "        * [Bar chart](#bar-chart)\n",
    "        * [Relative importance of each feature](#relative-importance)\n",
    "        * [Map visualization](#map-visualization)\n",
    "    * [Plan for milestone 3](#plan-ms-3)\n",
    "* [Milestone 3](#milestone-3)\n",
    "    * [Word Cloud](#word-cloud)\n",
    "    * [NetworkX](#pie-charts)\n",
    "    * [Holoviews circles](#holoviews)\n",
    "    * [Sentiment analysis](#sentiment)\n",
    "        * [Democrats](#sentiment-democrat)\n",
    "        * [Republicans](#sentiment-republican)\n",
    "    * [Pie charts](#pie-charts)\n",
    "    * [Venn diagram](#venn-diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import folium\n",
    "import ipywidgets as ipw\n",
    "import html\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Mimmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Ben\n",
    "import rake_nltk\n",
    "from rake_nltk import Rake\n",
    "from networkx.algorithms import bipartite\n",
    "from matplotlib_venn_wordcloud import venn2_wordcloud\n",
    "import plotly\n",
    "import pygal\n",
    "from ipywidgets import HTML\n",
    "import base64\n",
    "import branca.colormap as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2 <a class=\"anchor\" id=\"milestone-2\"></a>\n",
    "\n",
    "### Loading phase <a class=\"anchor\" id=\"loading-phase\"></a>\n",
    "\n",
    "The liar dataset is composed of three different datasets, namely `test.tsv`, `train.tsv` and `valid.tsv`, which we first load into Pandas dataframes with corresponding names. All of these datasets have exactly the same schema. The purpose of having three different datasets is to do machine learning, which is not the goal of this project. In consequence, we will combine all three dataframes `test`, `train` and `valid` into a single one, `liar`.\n",
    "\n",
    "The schema of these datasets is outlined below. There is a total of 14 columns and each row refers to a statement.\n",
    "\n",
    "> Column 1: ID of the statement ([ID].json).\n",
    "> \n",
    "> Column 2: Label.\n",
    "> \n",
    "> Column 3: Statement.\n",
    "> \n",
    "> Column 4: Subject(s).\n",
    "> \n",
    "> Column 5: Speaker.\n",
    "> \n",
    "> Column 6: Speaker's job title.\n",
    "> \n",
    "> Column 7: State info.\n",
    "> \n",
    "> Column 8: Party affiliation.\n",
    "> \n",
    "> Column 9: Barely true counts.\n",
    "> \n",
    "> Column 10: False counts.\n",
    "> \n",
    "> Column 11: Half true counts.\n",
    "> \n",
    "> Column 12: Mostly true counts.\n",
    "> \n",
    "> Column 13: Pants on fire counts.\n",
    "> \n",
    "> Column 14: Context (venue / location of the speech or statement).\n",
    "\n",
    "In order to reduce the size of the dataframe, we could drop the columns which we do not need. Note that this is not required because the dataset is rather small, but could be done simply for the sake of convenience. However, we will probably need all columns, so no action will be taken there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "SCHEMA = ['statement_id', 'label', 'statement', 'subject', \n",
    "          'speaker', 'profession', 'state', 'party', \n",
    "          'barely_true', 'false', 'half_true', \n",
    "          'mostly_true', 'pants_on_fire', 'context']\n",
    "\n",
    "# Load the datasets into pandas dataframes\n",
    "test = pd.read_csv(DATA_DIR + 'test.tsv', delimiter='\\t', header=None, names=SCHEMA, index_col=False)\n",
    "train = pd.read_csv(DATA_DIR + 'train.tsv', delimiter='\\t', header=None, names=SCHEMA, index_col=False)\n",
    "valid = pd.read_csv(DATA_DIR + 'valid.tsv', delimiter='\\t', header=None, names=SCHEMA, index_col=False)\n",
    "\n",
    "\n",
    "# Combine the three dataframes into one\n",
    "liar = pd.concat([train, test, valid], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "liar.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the three datasets are different (which they should based on their purpose):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar['statement_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning phase <a class=\"anchor\" id=\"cleaning-phase\"></a>\n",
    "\n",
    "This dataset id very clean by nature because it is not just a collection of data. Indeed, it was intended for use by other data scientists as a benchmark dataset and it will therefore not need many data cleaning operations. We can start by checking if every row is complete or if there are any NaNs or missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that most of the columns, more precisely column 4 to 15, do have missing values. However, we will not drop every row where there are NaNs because the other fields in these rows might still be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now investigate how the data is formatted to check for potential inconcistencies. We can start with the column concerning the state where the statement was made, because we can have a fairly good idea of what it should contain (i.e. names of locations like states or countries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar.state.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are inconcistencies in the use of upper case (e.g. `Ohio` and `ohio`) and problems with trailing spaces (e.g. `Georgia` and `Georgia  `). We can take care of these issues by making everything lower case and removing leading and trailing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    '''\n",
    "    Replaces upper case letters by lower case ones and removes leading and trailing spaces.\n",
    "    :param s: str\n",
    "    :return: s\n",
    "    '''\n",
    "    if isinstance(s, str):\n",
    "        s = s.lower()\\\n",
    "             .strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows before dropping \"all Nans\" rows\n",
    "print('Number of unique entities in \"state\" before applying \"clean\": %s'%liar.state.unique().shape[0])\n",
    "\n",
    "liar['state'] = liar['state'].apply(clean)\n",
    "\n",
    "# Number of rows before dropping \"all Nans\" rows\n",
    "print('Number of unique entities in \"state\" after  applying \"clean\": %s'%liar.state.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are still other issues with the data, but we will not take care of them. For example, there are spelling mistakes (e.g. `virgiia` instead of `virginia`). We will ignore them because we can assume that these spelling mistakes will concern only one statement, which means it is not worth taking the time to take care of that kind of issues.\n",
    "\n",
    "In order to clean the other columns in liar, we will use the same function as above, `clean`. We can indeed assume that they might suffer from the same inconsistencies. It is important to point out that we will not touch to the column `statement` because we want to keep all statements as they were originally written. If we were to modify them, then all conclusions from the analysis of the statements themselves would loose credibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in liar.columns:\n",
    "    if not (col_name == 'statement'):\n",
    "        liar[col_name] = liar[col_name].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a final look at our cleaned `liar` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "liar.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading additional files <a class=\"anchor\" id=\"loading-additional-files\"></a>\n",
    "\n",
    "In order to create maps using folium displaying content from the `liar` dataframe, a new column with the abbreviations of the states will be added. The pairs state name/abbreviations are all contained in the file `states_abbreviation.csv`, which can be downloaded [here](http://www.fonz.net/blog/archives/2008/04/06/csv-of-states-and-state-abbreviations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file 'states_abbreviation.csv' into a dataframe\n",
    "states_abbreviation = pd.read_csv(DATA_DIR + 'states_abbreviation.csv')\n",
    "\n",
    "# Make sure we have the same format than in liar\n",
    "states_abbreviation['State'] = states_abbreviation['State'].apply(clean)\n",
    "\n",
    "i = 0\n",
    "for state_name in liar['state']:\n",
    "    temp = states_abbreviation.Abbreviation[state_name==states_abbreviation.State].values\n",
    "    if temp.size != 0:\n",
    "        liar.at[i, 'state_abbreviation'] = temp[0]\n",
    "    else:\n",
    "        liar.at[i, 'state_abbreviation'] = np.nan\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "liar.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will also load two additional datasets, `pop-urban-pct-historical.xls` and `federalelections2016.xlsx`, which will have the purpose of highlighting potential explanations for the distribution of fake news across America. They are available for download respectively [there](https://www.icip.iastate.edu/tables/population/urban-pct-states) and [there](https://transition.fec.gov/pubrec/electionresults.shtml). For the same reason as earlier, we will add a column with the abbreviations of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pop-urban-pct-historical.xls\n",
    "states_urban = pd.read_excel(DATA_DIR + 'pop-urban-pct-historical.xls', sheet_name='States',\n",
    "                             header=5, usecols='B:G,I:M,O:P')\n",
    "states_urban = states_urban[1:52]\n",
    "\n",
    "# Make sure we have the same format than in states_abbreviation\n",
    "states_urban['Area Name'] = states_urban['Area Name'].apply(clean)\n",
    "\n",
    "# Add the states abbreviations\n",
    "i = 1\n",
    "for state_name in states_urban['Area Name']:\n",
    "    temp = states_abbreviation.Abbreviation[state_name==states_abbreviation.State].values\n",
    "    if temp.size != 0:\n",
    "        states_urban.at[i, 'state_abbreviation'] = temp[0]\n",
    "    else:\n",
    "        states_urban.at[i, 'state_abbreviation'] = np.nan\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "states_urban.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load federalelections2016.xlsx\n",
    "states_election = pd.read_excel(DATA_DIR + 'federalelections2016.xlsx', sheet_name=15,\n",
    "                                header=6, usecols='A:B,E,H,K,N,Q')\n",
    "states_election = states_election[1:52]\n",
    "states_election = states_election.rename(columns={\"CLINTON\": 2016, \"OBAMA\": 2012,\n",
    "                                                  \"OBAMA.1\": 2008, \"KERRY\": 2004,\n",
    "                                                  \"GORE\": 2000, \"CLINTON.1\": 1996})\n",
    "\n",
    "# Make sure we have the same format than in states_abbreviation\n",
    "states_election['STATE'] = states_election['STATE'].apply(clean)\n",
    "\n",
    "# Add the states abbreviations\n",
    "i = 1\n",
    "for state_name in states_election['STATE']:\n",
    "    temp = states_abbreviation.Abbreviation[state_name==states_abbreviation.State].values\n",
    "    if temp.size != 0:\n",
    "        states_election.at[i, 'state_abbreviation'] = temp[0]\n",
    "    else:\n",
    "        states_election.at[i, 'state_abbreviation'] = np.nan\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "states_election.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis <a class=\"anchor\" id=\"data-analysis\"></a>\n",
    "\n",
    "After the initialization with the loading and cleaning phases, we can start the data anlysis _per se_. First, we will create a simple bar chart showing the number of statements made in each states. Then, we will create a visualization tool to understand which feature plays an important role. Finally, we will look at the geographical distribution of fake news and try to correlate it with additional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar chart <a class=\"anchor\" id=\"bar-chart\"></a>\n",
    "\n",
    "The goal here is to get a first idea of how many statements were made in each states. For that, we will make a stacked bar chart showing the number of statements for every label and for each states. Note that we will consider here only american states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe with the number of statements for each state and for each label\n",
    "state_count = liar.dropna(subset=['state_abbreviation'])\\\n",
    "                  .groupby(['state','label'])\\\n",
    "                  .count()\\\n",
    "                  .reset_index()\\\n",
    "                  .filter(['state', 'label', 'statement_id'])\\\n",
    "                  .rename(columns={'statement_id':'#'})\n",
    "\n",
    "# Get a dataframe for each label with the number of statements for each state\n",
    "df_total       = liar.dropna(subset=['state_abbreviation']).groupby('state').count().filter(['statement_id']).rename(columns={'statement_id':'#_total'})\n",
    "df_pants_fire  = state_count[state_count['label']=='pants-fire'] .drop('label', axis=1).set_index('state')\n",
    "df_false       = state_count[state_count['label']=='false']      .drop('label', axis=1).set_index('state')\n",
    "df_barely_true = state_count[state_count['label']=='barely-true'].drop('label', axis=1).set_index('state')\n",
    "df_half_true   = state_count[state_count['label']=='half-true']  .drop('label', axis=1).set_index('state')\n",
    "df_mostly_true = state_count[state_count['label']=='mostly-true'].drop('label', axis=1).set_index('state')\n",
    "df_true        = state_count[state_count['label']=='true']       .drop('label', axis=1).set_index('state')\n",
    "\n",
    "# Join all dataframes together\n",
    "temp1 = df_total.join(df_pants_fire, how='outer')\\\n",
    "                .join(df_false, how='outer', rsuffix=' false')\\\n",
    "                .join(df_barely_true, how='outer', rsuffix=' barely true')\\\n",
    "                .join(df_half_true, how='outer', rsuffix=' half true')\\\n",
    "                .join(df_mostly_true, how='outer', rsuffix=' mostly true')\\\n",
    "                .join(df_true, how='outer', rsuffix=' true')\\\n",
    "                .rename(columns={'#':'# pants on fire'})\\\n",
    "                .fillna(0)\\\n",
    "                .sort_values(by=['#_total'], ascending=False)\\\n",
    "                .drop('#_total', axis=1)\n",
    "\n",
    "# Create a stacked plot\n",
    "temp1.plot.bar(stacked=True, title='Number of statements made in each state', figsize=(20,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of statements is varying greatly between states. It is uncertain whether this distribution is representative of the real distribution or results from the way the statements are collected. This might make the comparison between states uneasy if not totally meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative importance of each feature <a class=\"anchor\" id=\"relative-importance\"></a>\n",
    "\n",
    "In order to investigate and visualize the data contained in `liar`, we can create a widget which takes as inputs each of the different metadata associated to a statement and returns as output the number of statements which have this set of metadata. This will help understanding how important every features (like profession, subject, etc.) of the statements are and also help finding interesting patterns.\n",
    "\n",
    "Before we can do that, we have to define a function which will be able to retrieve only the most relevant and occurent values for each feature. For example, for the parties, we will care about the Republicans and the Democrats but less about other minor entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_x(col_name, x, sorted=False):\n",
    "    '''\n",
    "    Returns a sorted (if needed) list of the top x entities with the most occurencies in the column specified by col_name.\n",
    "    :param col_name: str, x: int, sorted: bool\n",
    "    :return: top_ten\n",
    "    '''\n",
    "    # Get the top x\n",
    "    top_x = liar[['statement_id', col_name]].groupby(col_name)\\\n",
    "                                            .count()\\\n",
    "                                            .sort_values('statement_id', ascending=False)\\\n",
    "                                            .head(x)\\\n",
    "                                            .index\\\n",
    "                                            .values\\\n",
    "                                            .astype('str')\n",
    "    \n",
    "    # Create an array of tupple where the first element is like the second but its dashes are replaced by spaces and each word is capitalized\n",
    "    top_x = [(s.replace('-', ' ').title(), s) for s in top_x]\n",
    "    \n",
    "    # If sorted is True, sort the list by alphabetical order\n",
    "    if sorted:\n",
    "        top_x.sort(key=lambda x: x[0])\n",
    "    return top_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need in order to build our widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of statements\n",
    "nb_tot = liar.shape[0]\n",
    "\n",
    "# Replace all NaNs by 'NA' to avoid categorizing statements as excluded when they should be included\n",
    "liarNA = liar.fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def label_proportion(toggle_dots, datapoints_per_dot, label, subject, speaker, profession, state, party, context):\n",
    "    '''\n",
    "    Print the proportion of statements which have the properties specified by the inputs.\n",
    "    :param toggle_dots: bool, datapoints_per_dot: int, label-context: str\n",
    "    :return: None\n",
    "    '''\n",
    "    # In case \"All <input>\" is required, replace the corresponding input by the column it corresponds to in liarNA\n",
    "    # so that the filter wrt this input contains only \"True\" values. \n",
    "    if subject == 'all_subjects':\n",
    "        subject = liarNA.subject\n",
    "    if speaker == 'all_speakers':\n",
    "        speaker = liarNA.speaker\n",
    "    if profession == 'all_professions':\n",
    "        profession = liarNA.profession\n",
    "    if state == 'all_states':\n",
    "        state = liarNA.state\n",
    "    if party == 'all_parties':\n",
    "        party = liarNA.party\n",
    "    if context == 'all_contexts':\n",
    "        context = liarNA.context\n",
    "    \n",
    "    # Compute the filters to keep only statements having the properties specified by the inputs\n",
    "    filter_all_but_labels = (liarNA.subject==subject)       & (liarNA.speaker==speaker) & \\\n",
    "                            (liarNA.profession==profession) & (liarNA.state==state)     & \\\n",
    "                            (liarNA.party==party)           & (liarNA.context==context)\n",
    "    \n",
    "    filter_pants_on_fire = (liarNA.label=='pants-fire')  & filter_all_but_labels\n",
    "    filter_false         = (liarNA.label=='false')       & filter_all_but_labels\n",
    "    filter_barely_true   = (liarNA.label=='barely-true') & filter_all_but_labels\n",
    "    filter_half_true     = (liarNA.label=='half-true')   & filter_all_but_labels\n",
    "    filter_mostly_true   = (liarNA.label=='mostly-true') & filter_all_but_labels\n",
    "    filter_true          = (liarNA.label=='true')        & filter_all_but_labels\n",
    "    \n",
    "    # Apply the filters and count the number of remaining statements for each label.\n",
    "    if label == 'all_labels':\n",
    "        nb_pants_on_fire = round(liarNA[filter_pants_on_fire].shape[0] / datapoints_per_dot)\n",
    "        nb_false         = round(liarNA[filter_false]        .shape[0] / datapoints_per_dot)\n",
    "        nb_barely_true   = round(liarNA[filter_barely_true]  .shape[0] / datapoints_per_dot)\n",
    "        nb_half_true     = round(liarNA[filter_half_true]    .shape[0] / datapoints_per_dot)\n",
    "        nb_mostly_true   = round(liarNA[filter_mostly_true]  .shape[0] / datapoints_per_dot)\n",
    "        nb_true          = round(liarNA[filter_true]         .shape[0] / datapoints_per_dot)\n",
    "    else:\n",
    "        nb_pants_on_fire = round(liarNA[filter_pants_on_fire & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_false         = round(liarNA[filter_false         & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_barely_true   = round(liarNA[filter_barely_true   & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_half_true     = round(liarNA[filter_half_true     & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_mostly_true   = round(liarNA[filter_mostly_true   & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "        nb_true          = round(liarNA[filter_true          & (liarNA.label==label)].shape[0] / datapoints_per_dot)\n",
    "    \n",
    "    # Count the number of statements which does not have the properties specified by the inputs\n",
    "    nb_others = round(nb_tot/datapoints_per_dot - (nb_pants_on_fire + nb_false + nb_barely_true +\n",
    "                                                   nb_half_true + nb_mostly_true + nb_true))\n",
    "    \n",
    "    # Print the legend\n",
    "    print('\\033[1mThere is a total of %s statements, out of which %s satisfy your requirements.\\033[0m'%(nb_tot, nb_tot-nb_others*datapoints_per_dot))\n",
    "    print('\\n\\033[41m    \\033[0m Pants on fire    ' +\n",
    "            '\\033[42m    \\033[0m False    '         +\n",
    "            '\\033[43m    \\033[0m Barely true    '   +\n",
    "            '\\033[44m    \\033[0m Half true    '     +\n",
    "            '\\033[45m    \\033[0m Mostly true    '   +\n",
    "            '\\033[46m    \\033[0m True    '          +\n",
    "            '\\033[40m    \\033[0m Excluded\\n')\n",
    "    \n",
    "    # Print colored areas proportional to the number of statements satisfying the requirements\n",
    "    if toggle_dots:# With dots\n",
    "        print('\\033[41m' + html.unescape(nb_pants_on_fire*'&#x25CF') + '\\033[0m' +\n",
    "              '\\033[42m' + html.unescape(nb_false*'&#x25CF')         + '\\033[0m' +\n",
    "              '\\033[43m' + html.unescape(nb_barely_true*'&#x25CF')   + '\\033[0m' +\n",
    "              '\\033[44m' + html.unescape(nb_half_true*'&#x25CF')     + '\\033[0m' +\n",
    "              '\\033[45m' + html.unescape(nb_mostly_true*'&#x25CF')   + '\\033[0m' +\n",
    "              '\\033[46m' + html.unescape(nb_true*'&#x25CF')          + '\\033[0m' +\n",
    "              '\\033[40m' + html.unescape(nb_others*'&#x25CF')        + '\\033[0m')\n",
    "    else:# Without dots\n",
    "        print('\\033[41m' + nb_pants_on_fire*' ' + '\\033[0m' +\n",
    "              '\\033[42m' + nb_false*' '         + '\\033[0m' +\n",
    "              '\\033[43m' + nb_barely_true*' '   + '\\033[0m' +\n",
    "              '\\033[44m' + nb_half_true*' '     + '\\033[0m' +\n",
    "              '\\033[45m' + nb_mostly_true*' '   + '\\033[0m' +\n",
    "              '\\033[46m' + nb_true*' '          + '\\033[0m' +\n",
    "              '\\033[40m' + nb_others*' '        + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a widget for the function above (label_proportion)\n",
    "ipw.interact(label_proportion,\n",
    "             toggle_dots = ipw.widgets.ToggleButton(value=False, description='Toggle dots', button_style='success', tooltip='Show/hide the dots representing each statements'),\n",
    "             datapoints_per_dot = ipw.widgets.IntSlider(value=1., min=1., max=10, description='#statements per dot', style={'description_width': 'initial'}),\n",
    "             label      = [('All labels', 'all_labels'), ('Pants on fire', 'pants-fire'),\n",
    "                           ('False', 'false'), ('Barely true', 'barely-true'),\n",
    "                           ('Half true', 'half-true'), ('Mostly true', 'mostly-true'),\n",
    "                           ('True', 'true')],\n",
    "             subject    = [('All subjects',    'all_subjects'   )] + find_top_x('subject',    10, sorted=True),\n",
    "             speaker    = [('All speakers',    'all_speakers'   )] + find_top_x('speaker',     5, sorted=False),\n",
    "             profession = [('All professions', 'all_professions')] + find_top_x('profession', 10, sorted=True),\n",
    "             state      = [('All states',      'all_states'     )] + find_top_x('state',      99, sorted=True),\n",
    "             party      = [('All parties',     'all_parties'    )] + find_top_x('party',       5, sorted=False),\n",
    "             context    = [('All contexts',    'all_contexts'   )] + find_top_x('context',    10, sorted=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map visualization <a class=\"anchor\" id=\"map-visualization\"></a>\n",
    "\n",
    "As we have information concerning where each statement was made, we can visualize their distribution geographically on a map. This will allow us to determine whether there are states where politicians tend to lie more. These states (if any) would be more vulnerable to potentially harmful consequences of fake news and thus identifying them is an important task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geo json file to display the states\n",
    "states_geo_json = json.load(open(DATA_DIR + r'us-states.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_map_label(label):\n",
    "    '''\n",
    "    Plots a map of the USA indicating the number of statements per states for a selected label.\n",
    "    :param label: str\n",
    "    :return: None\n",
    "    '''\n",
    "    # Filter out all statements with labels different from label\n",
    "    filter_label = liar.label == label\n",
    "    \n",
    "    # Count the number of remaining statements per state\n",
    "    state_count = liar[filter_label].groupby('state_abbreviation')\\\n",
    "                                    .count()\\\n",
    "                                    .reset_index()\\\n",
    "                                    .filter(['state_abbreviation', 'statement_id'])\n",
    "    \n",
    "    # Add the states where no statements were made (so that they do not appear with the darkest color on the map)\n",
    "    for state in states_abbreviation['Abbreviation']:\n",
    "        if not liar[filter_label]['state_abbreviation'].str.contains(state, regex=False).any():\n",
    "            state_count = state_count.append(pd.DataFrame([(state, 0)],\n",
    "                                             columns=['state_abbreviation', 'statement_id']))\n",
    "    \n",
    "    # Create a map centered around the USA\n",
    "    m_usa = folium.Map([43,-100], tiles='cartodbpositron', zoom_start=4)\n",
    "    \n",
    "    # Add a filter to the map indicating the number of statements in each state\n",
    "    m_usa.choropleth(geo_data=states_geo_json, data=state_count,\n",
    "                    columns=['state_abbreviation', 'statement_id'],\n",
    "                    key_on='feature.id',\n",
    "                    fill_color='YlOrRd', fill_opacity=0.7, line_opacity=1,\n",
    "                    legend_name=('Number of %s statements in each state'%label),\n",
    "                    highlight=True)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m_usa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to just visualizing the number of statements per state, we can also compare the results with the data contained in the two additional datasets which we loaded earlier, `pop-urban-pct-historical.xls` and `federalelections2016.xlsx`. For that, we will display another map right below the previous one in order to allow a direct comparison between the two maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_map_comparison(select_data):\n",
    "    '''\n",
    "    Plots a map of the USA with either information on urbanity or elections outcome.\n",
    "    :param select_data: str\n",
    "    :return: None\n",
    "    '''\n",
    "    # Select the right dataset\n",
    "    if select_data[0:-4] == 'urb_':\n",
    "        data = states_urban.filter(['state_abbreviation', int(select_data[-4:])])\n",
    "        legend = 'Percentage of the population living in urban areas'\n",
    "    if select_data[0:-4] == 'elec_':\n",
    "        data = states_election.filter(['state_abbreviation', int(select_data[-4:])])\n",
    "        legend = 'Percentage of vote for the democrats'\n",
    "    \n",
    "    # Create a map centered around the USA\n",
    "    m_usa = folium.Map([43,-100], tiles='cartodbpositron', zoom_start=4)\n",
    "    \n",
    "    # Add a filter to the map indicating the required data\n",
    "    m_usa.choropleth(geo_data=states_geo_json, data=data,\n",
    "                    columns=['state_abbreviation', int(select_data[-4:])],\n",
    "                    key_on='feature.id',\n",
    "                    fill_color='RdBu', fill_opacity=0.7, line_opacity=1,\n",
    "                    legend_name=legend,\n",
    "                    highlight=True)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a widget for the function plot_map_label\n",
    "ipw.interact(plot_map_label, label=[('Pants on fire', 'pants-fire'), ('False', 'false'), ('Barely true', 'barely-true'),\n",
    "                           ('Half true', 'half-true'), ('Mostly true', 'mostly-true'), ('True', 'true')]);\n",
    "\n",
    "# Create a widget for the function plot_map_comparison\n",
    "ipw.interact(plot_map_comparison, select_data=[('Urban percentage 2010', 'urb_2010'), ('Urban percentage 2000', 'urb_2010'),\n",
    "                                               ('Election 2016', 'elec_2016'), ('Election 2012', 'elec_2012'),\n",
    "                                               ('Election 2008', 'elec_2008'), ('Election 2004', 'elec_2004'),\n",
    "                                               ('Election 2000', 'elec_2000'), ('Election 2096', 'elec_1996')]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan for milestone 3 <a class=\"anchor\" id=\"plan-ms-3\"></a>\n",
    "\n",
    "The dataset that we have chosen is quite rich with information, given the statement texts (the corpus), and we plan to use this to go beyond the labels provided by the dataset. We will use NLTK (Natural Language Toolkit) and RAKE (Rapid Automatic Keyword Extraction) to analyze the corpus; find key phrases, discover buzzwords, and so on.\n",
    "\n",
    "We also want to explore and visualize the data by making a word cloud (using WordCloud) in Python.\n",
    "\n",
    "Furthermore, we would like to explore the relationships between the speakers; the inter-person lies - who lies about who? In order to visualize this, we will use NetworkX to create a directed graph. Perhaps we will discover a bipartite graph showing that republicans only lie about democrats, and vice versa?\n",
    "\n",
    "Additionally, by using a Venn diagram, we would like to see if there are topics that only republicans, or only democrats, lie about, and which topics that are subjected to lies by both parties.\n",
    "\n",
    "Finally, we would like to show the topic most prominent to lies in each state, by using Folium to create a map with interactive markers. This is because we want to see which topic that is most often the subject of fake news in each state, as some topics are likely to be of higher importance in some states. For example, discussion of immigration laws, or gun control, might be more present in states with a lot of immigration and guns. We would like to examine these connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3 <a class=\"anchor\" id=\"milestone-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mimmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud <a class=\"anchor\" id=\"word-cloud\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the statements into a single string\n",
    "text = \" \".join(statement for statement in liar.statement)\n",
    "\n",
    "# Create a list with stopwords to remove from the word cloud\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(['Say', 'said', 'now', 'one', 'percent', 'Says', 'year', 'every',\\\n",
    "                  'less', 'first', 'day', 'make', 'time', 'even', 'last', 'back',\\\n",
    "                  'thats', 'went', 'come', 'use', 'without', 'still', 'dont', 'half',\\\n",
    "                  'got', 'take', 'two', 'almost', 'come', 'nearly', 'want', 'go', 'going',\\\n",
    "                  'new', 'made', 'third', 'used', 'month', 'never', 'many', 'three',\\\n",
    "                  'know', 'four', 'us', 'per', 'put', 'today', 'highest', 'give', 'Rep', \\\n",
    "                  'actually', 'times', 'lowest', 'single', 'already', 'top', 'think', 'including',\\\n",
    "                  'second', 'took', 'much', 'called', 'largest', 'show', 'didnt', 'five', 'wants', \\\n",
    "                  'theres', 'keep', 'higher', 'need', 'hour', 'least', 'days', 'part', 'among',\\\n",
    "                  'away', 'week', 'will', 'million', 'people', 'person'])\n",
    "\n",
    "# Create a mask in the shape of the United States\n",
    "usa_mask = np.array(Image.open(DATA_DIR + \"usa_light_blue.png\"))\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "wordcloud = WordCloud(max_words = 140, stopwords = stopwords, mode = \"RGBA\", background_color = None,\\\n",
    "                      width = 800, height = 400, mask = usa_mask, contour_width = 0, contour_color = 'blue',\\\n",
    "                      colormap = 'Dark2').generate(text)\n",
    "\n",
    "# Display the generated image\n",
    "a = plt.figure(figsize = (30,40))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x = 0, y = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetworkX <a class=\"anchor\" id=\"networkx\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from community import community_louvain\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only interested in Democrats and Republicans\n",
    "node_list = liar[['speaker', 'party', 'statement', 'label']]\n",
    "node_list = node_list.loc[node_list['party'].isin(['democrat', 'republican'])]\n",
    "node_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = node_list.loc[node_list['label'].isin(['false', 'pants-fire'])]\n",
    "node_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list['speaker'] = node_list['speaker'].str.replace('-', ' ')\n",
    "node_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list['statement'] = node_list['statement'].str.lower()\n",
    "node_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_speakers = node_list[['speaker', 'party']].drop_duplicates()\n",
    "unique_speakers.reset_index(inplace=True, drop=True)\n",
    "print('Number of unique speakers:', len(unique_speakers))\n",
    "unique_speakers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list.iat[3,0]\n",
    "unique_speakers.index[unique_speakers['speaker'] == 'nancy pelosi'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = pd.DataFrame(columns = ['Source', 'SourceID', 'Target', 'TargetID'])\n",
    "for speaker in node_list['speaker']:\n",
    "    index = 0\n",
    "    for statement in node_list['statement']:\n",
    "        if speaker in statement:\n",
    "            target = speaker\n",
    "            source = node_list.iat[index, 0]\n",
    "            target_id = unique_speakers.index[unique_speakers['speaker'] == target].tolist()[0]\n",
    "            source_id = unique_speakers.index[unique_speakers['speaker'] == source].tolist()[0]\n",
    "            source_target = pd.DataFrame([[source, source_id, target, target_id]], columns=['Source', 'SourceID', 'Target', 'TargetID'])\n",
    "            edge_list = edge_list.append(source_target, ignore_index = True)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nx.from_pandas_edgelist(edge_list, 'Source', 'Target', edge_attr = None, create_using = nx.DiGraph())\n",
    "print(nx.info(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it out\n",
    "pos = nx.spring_layout(net, k=0.2)\n",
    "ec = nx.draw_networkx_edges(net, pos, alpha=0.4)\n",
    "nc = nx.draw_networkx_nodes(net, pos, nodelist=net.nodes(), node_color='r', cmap=plt.cm.jet, node_shape='.')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_list[['speaker', 'party']].drop_duplicates()\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['speaker'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nodes.set_index('speaker')\n",
    "nodes = nodes.reindex(net.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['party'] = pd.Categorical(nodes['party'])\n",
    "nodes['party'].cat.codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "nx.draw(net, with_labels=False, node_color=nodes['party'].cat.codes, cmap=plt.cm.bwr, node_size=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create three graphs: a bipartite graph showing only interactions between democrats and republicans, a graph showing the interactions inside the democratic party and the last one showing the interactions inside the republican party. This will be useful to illustrate the rivalry between the two parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "edges_democrats = []\n",
    "edges_republicans = []\n",
    "for i in edge_list.index:\n",
    "    speaker_src = edge_list['Source'][i]\n",
    "    speaker_trg = edge_list['Target'][i]\n",
    "    party_src = unique_speakers[unique_speakers['speaker'] == speaker_src]['party'].tolist()\n",
    "    party_trg = unique_speakers[unique_speakers['speaker'] == speaker_trg]['party'].tolist()\n",
    "    if party_src == party_trg:\n",
    "        if party_src[0] == 'democrat':\n",
    "            edges_democrats = edges_democrats + [(speaker_src, speaker_trg)]\n",
    "        elif party_src[0] == 'republican':\n",
    "            edges_republicans = edges_republicans + [(speaker_src, speaker_trg)]\n",
    "    else:\n",
    "        edges = edges + [(speaker_src, speaker_trg)]\n",
    "\n",
    "# Find all speakers for both parties\n",
    "unique_democrats = unique_speakers.where(unique_speakers['party']=='democrat')['speaker'].dropna().tolist()\n",
    "unique_republicans = unique_speakers.where(unique_speakers['party']=='republican')['speaker'].dropna().tolist()\n",
    "\n",
    "# Define the node color (blue for democrats and red for republicans)\n",
    "node_color = np.asarray(len(unique_democrats)*[0] + len(unique_republicans)*[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_democrats = unique_speakers.where(unique_speakers['party']=='democrat')['speaker'].dropna().tolist()\n",
    "unique_republicans = unique_speakers.where(unique_speakers['party']=='republican')['speaker'].dropna().tolist()\n",
    "\n",
    "DR = nx.DiGraph()\n",
    "\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "DR.add_nodes_from(unique_democrats, bipartite=0)\n",
    "DR.add_nodes_from(unique_republicans, bipartite=1)\n",
    "\n",
    "# Add edges only between nodes of opposite node sets\n",
    "DR.add_edges_from(edges)\n",
    "\n",
    "# Define a different location for nodes of republicans and democrats\n",
    "top_nodes = {n for n, d in DR.nodes(data=True) if d['bipartite']==0}\n",
    "bottom_nodes = set(DR) - top_nodes\n",
    "pos = dict()\n",
    "pos.update( (n, (1, i)) for i, n in enumerate(top_nodes) ) # put nodes from X at x=1\n",
    "pos.update( (n, (2, i)) for i, n in enumerate(bottom_nodes) ) # put nodes from Y at x=2\n",
    "\n",
    "# Plot the graph\n",
    "fig_DR = plt.figure(figsize=(12,8))\n",
    "nx.draw(DR, pos=pos, node_color=node_color, cmap=plt.cm.bwr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = nx.DiGraph()\n",
    "\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "D.add_nodes_from(unique_democrats)\n",
    "\n",
    "# Add edges only between nodes of opposite node sets\n",
    "D.add_edges_from(edges_democrats)\n",
    "\n",
    "# Plot the graph\n",
    "fig_D = plt.figure(figsize=(5,4))\n",
    "nx.draw(D, node_color='blue', node_size = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = nx.DiGraph()\n",
    "\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "R.add_nodes_from(unique_democrats)\n",
    "\n",
    "# Add edges only between nodes of opposite node sets\n",
    "R.add_edges_from(edges_democrats)\n",
    "\n",
    "# Plot the graph\n",
    "fig_R = plt.figure(figsize=(5,4))\n",
    "nx.draw(R, node_size = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoloViews Circle <a class=\"anchor\" id=\"holoviews\"></a>\n",
    "\n",
    "We want to take a closer look at the 20 Democrats/Republicans with the most 'pants-fire' and 'false' statements; to visualize the amount of lies, and the relationships between the politicians in a graph. To do so, we use a `Chord` element in HoloViews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only interested in Democrats and Republicans\n",
    "statements = liar[['speaker', 'party', 'statement', 'label']]\n",
    "statements = statements.loc[statements['party'].isin(['democrat', 'republican'])]\n",
    "statements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements['speaker'] = statements['speaker'].str.replace('-', ' ')\n",
    "statements['statement'] = statements['statement'].str.lower()\n",
    "statements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = statements[['speaker', 'party']].drop_duplicates()\n",
    "node_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that each speaker is affiliated with only one party\n",
    "node_list['speaker'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list['Count'] = 1\n",
    "edge_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the lies between speakers\n",
    "edge_count = edge_list.groupby(['Source', 'Target']).Count.count().reset_index()\n",
    "nodes = hv.Dataset(node_list, 'speaker', 'party')\n",
    "chord = hv.Chord((edge_count, nodes), ['Source', 'Target'], ['Count'])\n",
    "\n",
    "# Select the top 20 liars\n",
    "top_20 = list(edge_list.groupby('Source').count().sort_values('Count').iloc[-20:].index.values)\n",
    "top_20_liars = chord.select(speaker=top_20, selection_mode='nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Chord [edge_color_index='Source' label_index='speaker' color_index='Target' width=800 height=800]\n",
    "%%opts Chord (cmap='Category20' edge_cmap='Category20')\n",
    "top_20_liars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis <a class=\"anchor\" id=\"sentiment\"></a>\n",
    "\n",
    "We first wanted to see if there were any differences between truths and lies with regard to sentiment; e.g., would lies turn out be more negative, and truths more positive. Or perhaps the other way around? We used Spacy and a sentiment analyzer to explore this, but quickly observed that there were approximately the same number of positve and negative statements for both truths and lies, and more neutral statements in both cases. This was not a very interesting obersvation, but a little surprising, as we would expect political statements to have a somewhat strong sentiment, either positive or negative.\n",
    "\n",
    "Since our first analysis didn't give any results of importance, we then analyzed the sentiment of the statements made by Democrats and Republicans, respectively. Sadly, this analysis yielded approximately the same (uninteresting) results, and so we decided to try a more narrow approach; to analyze the sentiment of a few isolated subjects with respect to Democrats and Republicans. More specifically; health care and immigration. Neither yielded particularly interesting results, and so we're choosing to leave the sentiment analysis out of the data story, altogether. However, we leave the code here, with the health care approach, as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os, codecs, string\n",
    "\n",
    "#NLP library\n",
    "import spacy\n",
    "\n",
    "#Vader\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract only the statements discussing health-care made by Democrats, and then by Republicans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = liar[['statement', 'party', 'subject']]\n",
    "health_care = statements.loc[statements['subject'].isin(['health-care'])]\n",
    "\n",
    "democrats = health_care.loc[health_care['party'].isin(['democrat'])]\n",
    "republicans = health_care.loc[health_care['party'].isin(['republican'])]\n",
    "\n",
    "print('Statements on health care by democrats:', len(democrats))\n",
    "print('Statements on health care by republicans:', len(republicans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then turn the DataFrames into a corpus and a document for natural language processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_democrats = ' '.join(democrats['statement'])\n",
    "corpus_democrats[0:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_republicans = ' '.join(republicans['statement'])\n",
    "corpus_republicans[0:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "democrats_doc = nlp(corpus_democrats)\n",
    "\n",
    "republicans_doc = nlp(corpus_republicans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyser\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis of Health Care by Democrats <a class=\"anchor\" id=\"sentiment-democrat\"></a>\n",
    "For the sentiment analysis, we iterate through all the sentences, get a polarity score, and visualize with a histogram.\n",
    "\n",
    "We start by analyzing the statements made by Democrats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the positive sentiment of the Democrats\n",
    "positive_sent = []\n",
    "[positive_sent.append(analyzer.polarity_scores(sent.text)['pos']) for sent in democrats_doc.sents]\n",
    "plt.hist(positive_sent, bins=15)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 150])\n",
    "plt.xlabel('Positive sentiment')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the negative sentiment of the Democrats\n",
    "negative_sent = []\n",
    "[negative_sent.append(analyzer.polarity_scores(sent.text)['neg']) for sent in democrats_doc.sents]\n",
    "plt.hist(negative_sent, bins=15)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 150])\n",
    "plt.xlabel('Negative sentiment')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the comound sentiment for the Democrats\n",
    "total_sent = []\n",
    "[total_sent.append(analyzer.polarity_scores(sent.text)['compound']) for sent in democrats_doc.sents]\n",
    "plt.hist(total_sent, bins=15)\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Compound sentiment')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of sentences for each sentiment\n",
    "sents = [analyzer.polarity_scores(sent.text)['compound'] for sent in democrats_doc.sents]\n",
    "print('Number of positive sentences:', sum(np.array(sents)>=0.05))\n",
    "print('Number of negative sentences:', sum(np.array(sents)<=-0.05))\n",
    "print('Number of neutral sentences:', sum(np.abs(np.array(sents))<0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis of Health Care by Republicans  <a class=\"anchor\" id=\"sentiment-republican\"></a>\n",
    "\n",
    "We repeat the process for the Republicans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the positive sentiment for the Republicans\n",
    "positive_sent = []\n",
    "[positive_sent.append(analyzer.polarity_scores(sent.text)['pos']) for sent in republicans_doc.sents]\n",
    "plt.hist(positive_sent, bins=15)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 200])\n",
    "plt.xlabel('Positive sentiment')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the negative sentiment for the Republicans\n",
    "negative_sent = []\n",
    "[negative_sent.append(analyzer.polarity_scores(sent.text)['neg']) for sent in republicans_doc.sents]\n",
    "plt.hist(negative_sent, bins=15)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 200])\n",
    "plt.xlabel('Negative sentiment')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the compound sentiment for the truths\n",
    "total_sent = []\n",
    "[total_sent.append(analyzer.polarity_scores(sent.text)['compound']) for sent in republicans_doc.sents]\n",
    "plt.hist(total_sent, bins=15)\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Compound sentiment')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of sentences for each sentiment\n",
    "sents = [analyzer.polarity_scores(sent.text)['compound'] for sent in republicans_doc.sents]\n",
    "print('Number of positive sentences:', sum(np.array(sents)>=0.05))\n",
    "print('Number of negative sentences:', sum(np.array(sents)<=-0.05))\n",
    "print('Number of neutral sentences:', sum(np.abs(np.array(sents))<0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make some observations with regard to Democrats' and Republicans' discussion of health care. The pattern is the same for both parties; most sentences are positive, quite a few are neutral, and negative sentences are most rare. This surprised us a little, as these statements are from the time of Obama Care, and we would expect the Republicans to be more negative. However, we still don't find it interesting enough for it to be a part of the data story. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data in the liar dataframe\n",
    "liar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start by just checking out a statement\n",
    "row = liar.iloc[2, :]\n",
    "print(row.statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_statement(statement):\n",
    "    tokenized_statement = nltk.word_tokenize(row.statement)\n",
    "    return tokenized_statement\n",
    "\n",
    "tokenized_statement= tokenize_statement(row.statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_statement(tokenized_statement):\n",
    "    tagged_statement = nltk.pos_tag(tokenized_statement)\n",
    "    return tagged_statement\n",
    "\n",
    "tagged_statement = tag_statement(tokenized_statement)\n",
    "tagged_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(tokenized_statement):\n",
    "    \"\"\"\n",
    "    :param tokes: list of tokes without tags\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(tokenized_statement)\n",
    "        # When binary is sat to fasle we can see what part of the sentence that is\n",
    "        # a person, geographical location, organication, facility.....\n",
    "        # When true, we only get the EN (Named Entities) type\n",
    "        namedEnt = nltk.ne_chunk(tagged, binary=False)\n",
    "        namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    return namedEnt\n",
    "        \n",
    "chuncks = process_content(tokenized_statement)\n",
    "print(chuncks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_statement):\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokes without tags\n",
    "    :return lemma_list\n",
    "    \"\"\"\n",
    "    lemma_list = []\n",
    "    for i in tokenized_statement:\n",
    "        # Lemmatize noun (default)\n",
    "        lem = lemmatizer.lemmatize(i)\n",
    "        if lem == i:\n",
    "            # Lemmatize adjectiv\n",
    "            lem = lemmatizer.lemmatize(i, pos=\"a\")\n",
    "        if lem == i:\n",
    "            # Lemmatize verb\n",
    "            lem = lemmatizer.lemmatize(i, pos=\"v\")\n",
    "        # Could probably add more\n",
    "        lemma_list.append(lem)\n",
    "    return lemma_list\n",
    "\n",
    "lemmatizing(tokenized_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For next cell see this link [Stanford Named Entity Recognizer (NER)](https://nlp.stanford.edu/software/CRF-NER.shtml#Download). Is supposed to be better (accuracy) but a little slower than nltk's NER (by alot). ## Ben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stanford_tagger(tokenized_statement):\n",
    "    st = StanfordNERTagger('stanford-ner-2018-10-16\\classifiers\\english.muc.7class.distsim.crf.ser.gz',\n",
    "                           'stanford-ner-2018-10-16\\stanford-ner.jar',\n",
    "                           encoding='utf-8')\n",
    "\n",
    "    classified_text = st.tag(tokenized_statement)\n",
    "    return classified_text\n",
    "\n",
    "stanford_tagger(tokenized_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAKE\n",
    "\n",
    "Using the Rapid Automatic Keywords Extraction algorithm developped by [Rose et al.](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents) and implemented by [Vishwas B Sharma](https://pypi.org/project/rake-nltk/), we can easily extract keywords for each statements and their respective weight. They will all be stored as lists in the `liar` dataframe. For your information, a keyword can either be a single word or a short expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "\n",
    "keywords = []\n",
    "for s in liar['statement']:\n",
    "    r.extract_keywords_from_text(s)\n",
    "    keywords.append(r.get_ranked_phrases_with_scores()) # To get keyword phrases ranked highest to lowest.\n",
    "\n",
    "liar['keywords'] = keywords\n",
    "\n",
    "liar.keywords[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie charts <a class=\"anchor\" id=\"pie-charts\"></a>\n",
    "\n",
    "Some pie charts will be made in order to introduce the dataset at the beggining of the data story. This will help to present the different features and the distribution over these features of the statements. To make these charts, we will first need to compute (with the help of `label_proportion_mod` and `find_top_x_piechart`) how many statements were made for the top 10 items for each feature (e.g. for the feature `speaker`, how many statements Barack Obama, Donald Trump, and the rest of the top 10 speakers made)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of statements\n",
    "nb_tot = liar.shape[0]\n",
    "\n",
    "# Replace all NaNs by 'NA' to avoid categorizing statements as excluded when they should be included\n",
    "liarNA = liar.fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_proportion_mod(label, subject, speaker, profession, state, party, context):\n",
    "    '''\n",
    "    Print the proportion of statements which have the properties specified by the inputs.\n",
    "    :param toggle_dots: bool, datapoints_per_dot: int, label-context: str\n",
    "    :return: None\n",
    "    '''\n",
    "    # In case \"All <input>\" is required, replace the corresponding input by the column it corresponds to in liarNA\n",
    "    # so that the filter wrt this input contains only \"True\" values. \n",
    "    if subject == 'all_subjects':\n",
    "        subject = liarNA.subject\n",
    "    if speaker == 'all_speakers':\n",
    "        speaker = liarNA.speaker\n",
    "    if profession == 'all_professions':\n",
    "        profession = liarNA.profession\n",
    "    if state == 'all_states':\n",
    "        state = liarNA.state\n",
    "    if party == 'all_parties':\n",
    "        party = liarNA.party\n",
    "    if context == 'all_contexts':\n",
    "        context = liarNA.context\n",
    "    \n",
    "    # Compute the filters to keep only statements having the properties specified by the inputs\n",
    "    filter_all_but_labels = (liarNA.subject==subject)       & (liarNA.speaker==speaker) & \\\n",
    "                            (liarNA.profession==profession) & (liarNA.state==state)     & \\\n",
    "                            (liarNA.party==party)           & (liarNA.context==context)\n",
    "    \n",
    "    filter_pants_on_fire = (liarNA.label=='pants-fire')  & filter_all_but_labels\n",
    "    filter_false         = (liarNA.label=='false')       & filter_all_but_labels\n",
    "    filter_barely_true   = (liarNA.label=='barely-true') & filter_all_but_labels\n",
    "    filter_half_true     = (liarNA.label=='half-true')   & filter_all_but_labels\n",
    "    filter_mostly_true   = (liarNA.label=='mostly-true') & filter_all_but_labels\n",
    "    filter_true          = (liarNA.label=='true')        & filter_all_but_labels\n",
    "    \n",
    "    # Apply the filters and count the number of remaining statements for each label.\n",
    "    if label == 'all_labels':\n",
    "        nb_pants_on_fire = liarNA[filter_pants_on_fire].shape[0]\n",
    "        nb_false         = liarNA[filter_false]        .shape[0]\n",
    "        nb_barely_true   = liarNA[filter_barely_true]  .shape[0]\n",
    "        nb_half_true     = liarNA[filter_half_true]    .shape[0]\n",
    "        nb_mostly_true   = liarNA[filter_mostly_true]  .shape[0]\n",
    "        nb_true          = liarNA[filter_true]         .shape[0]\n",
    "    else:\n",
    "        nb_pants_on_fire = liarNA[filter_pants_on_fire & (liarNA.label==label)].shape[0]\n",
    "        nb_false         = liarNA[filter_false         & (liarNA.label==label)].shape[0]\n",
    "        nb_barely_true   = liarNA[filter_barely_true   & (liarNA.label==label)].shape[0]\n",
    "        nb_half_true     = liarNA[filter_half_true     & (liarNA.label==label)].shape[0]\n",
    "        nb_mostly_true   = liarNA[filter_mostly_true   & (liarNA.label==label)].shape[0]\n",
    "        nb_true          = liarNA[filter_true          & (liarNA.label==label)].shape[0]\n",
    "    \n",
    "    # Count the number of statements which does not have the properties specified by the inputs\n",
    "    nb_others = nb_tot - (nb_pants_on_fire + nb_false + nb_barely_true +\n",
    "                                                   nb_half_true + nb_mostly_true + nb_true)\n",
    "    nb_out = nb_tot-nb_others\n",
    "    \n",
    "    return nb_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_x_piechart(col_name, x, label='all_labels', subject='all_subjects',\n",
    "                        speaker='all_speakers', profession='all_professions',\n",
    "                        state='all_states', party='all_parties', context='all_contexts'):\n",
    "    '''\n",
    "    Returns a sorted (if needed) list of the top x entities with the most occurencies in the column specified by col_name.\n",
    "    :param col_name: str, x: int, sorted: bool\n",
    "    :return: top_ten\n",
    "    '''\n",
    "    # Get the top x\n",
    "    top_x = liar[['statement_id', col_name]].groupby(col_name)\\\n",
    "                                            .count()\\\n",
    "                                            .sort_values('statement_id', ascending=False)\\\n",
    "                                            .head(x)\\\n",
    "                                            .index\\\n",
    "                                            .values\\\n",
    "                                            .astype('str')\n",
    "    \n",
    "    pie_chart_data = [[i.replace('-', ' ').title(), label_proportion_mod(label=label, subject=subject, speaker=i,\n",
    "                                                                         profession=profession, state=state,\n",
    "                                                                         party=party, context=context)] for i in top_x]\n",
    "    nb_other = nb_tot\n",
    "    for _, nb in pie_chart_data:\n",
    "        nb_other = nb_other - nb\n",
    "    pie_chart_data = pie_chart_data + [['Others', nb_other]]    \n",
    "    return pie_chart_data\n",
    "\n",
    "# An exemple for the feature \"speaker\"\n",
    "find_top_x_piechart('speaker', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line of code above (used to find the top 10 for each feature) was run for all features and the results were stored in the following cell in order to retrieve it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 10 for each feature are all summarized here\n",
    "data_speaker = [['Barack Obama', 611],\n",
    "                ['Donald Trump', 343],\n",
    "                ['Hillary Clinton', 297],\n",
    "                ['Mitt Romney', 212],\n",
    "                ['John Mccain', 189],\n",
    "                ['Scott Walker', 183],\n",
    "                ['Chain Email', 178],\n",
    "                ['Rick Perry', 173],\n",
    "                ['Marco Rubio', 153],\n",
    "                ['Rick Scott', 150],\n",
    "                ['Others', 10302]]\n",
    "\n",
    "data_subject = [['Health Care', 474],\n",
    "                ['Taxes', 356],\n",
    "                ['Education', 309],\n",
    "                ['Elections', 304],\n",
    "                ['Immigration', 303],\n",
    "                ['Candidates Biography', 239],\n",
    "                ['Economy', 170],\n",
    "                ['Federal Budget', 152],\n",
    "                ['Economy,Jobs', 152],\n",
    "                ['Guns', 152],\n",
    "                ['Others', 10180]]\n",
    "\n",
    "data_profession = [['U.S. Senator', 923],\n",
    "                   ['President', 615],\n",
    "                   ['Governor', 537],\n",
    "                   ['U.S. Representative', 375],\n",
    "                   ['President Elect', 343],\n",
    "                   ['Presidential Candidate', 315],\n",
    "                   ['State Senator', 257],\n",
    "                   ['State Representative', 226],\n",
    "                   ['Former Governor', 213],\n",
    "                   ['Senator', 194],\n",
    "                   ['Others', 8793]]\n",
    "\n",
    "data_state = [['Texas', 1260],\n",
    "              ['Florida', 1237],\n",
    "              ['Wisconsin', 902],\n",
    "              ['New York', 831],\n",
    "              ['Illinois', 695],\n",
    "              ['Ohio', 589],\n",
    "              ['Georgia', 553],\n",
    "              ['Virginia', 513],\n",
    "              ['Rhode Island', 454],\n",
    "              ['Oregon', 310],\n",
    "              ['Others', 5447]]\n",
    "\n",
    "\n",
    "data_party = [['Republican', 5665],\n",
    "              ['Democrat', 4137],\n",
    "              ['None', 2181],\n",
    "              ['Organization', 264],\n",
    "              ['Independent', 180],\n",
    "              ['Newsmaker', 64],\n",
    "              ['Libertarian', 51],\n",
    "              ['Journalist', 49],\n",
    "              ['Activist', 45],\n",
    "              ['Columnist', 44],\n",
    "              ['Others', 111]]\n",
    "\n",
    "\n",
    "data_context = [['News Release', 318],\n",
    "                ['Interview', 287],\n",
    "                ['Press Release', 286],\n",
    "                ['Speech', 263],\n",
    "                ['Tv Ad', 229],\n",
    "                ['Tweet', 200],\n",
    "                ['Campaign Ad', 166],\n",
    "                ['Television Ad', 161],\n",
    "                ['Radio Interview', 128],\n",
    "                ['Debate', 118],\n",
    "                ['Others', 10635]]\n",
    "\n",
    "data_label = [['Pants on Fire', 1047],\n",
    "              ['False', 2507],\n",
    "              ['Barely True', 2103],\n",
    "              ['Half True', 2627],\n",
    "              ['Mostly True', 2454],\n",
    "              ['True', 2053]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Pygal chart\n",
    "custom_style = pygal.style.Style(\n",
    "  background='transparent',\n",
    "  plot_background='transparent',\n",
    "  foreground='black',\n",
    "  foreground_strong='black',\n",
    "  foreground_subtle='black',\n",
    "  tooltip_font_size='50',\n",
    "  font_family='Lora,\"Helvetica Neue\",Helvetica,Arial,sans-serif',\n",
    "  title_font_size=20,\n",
    "  legend_font_size=15,\n",
    "  opacity='.75',\n",
    "  opacity_hover='.1',\n",
    "  transition='400ms ease-in')\n",
    "\n",
    "pie_chart = pygal.Pie(inner_radius=0.4, style=custom_style)\n",
    "\n",
    "# add a title\n",
    "pie_chart.title = \"Speaker\"\n",
    "\n",
    "# add the data\n",
    "for label, data_points in data_speaker:\n",
    "    pie_chart.add(label, data_points)\n",
    "\n",
    "# Render the chart\n",
    "b64 = base64.b64encode(pie_chart.render())\n",
    "src = 'data:image/svg+xml;charset=utf-8;base64,'+b64.decode(\"utf-8\")\n",
    "HTML('<embed src={}></embed>'.format(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venn diagram <a class=\"anchor\" id=\"venn-diagram\"></a>\n",
    "\n",
    "To see if there are any differences between the subjects which either the democrats or the republicans talk about, we will create a Venn diagram showing the shared subjects and the ones which only one of the two polictical party mentions.\n",
    "\n",
    "In order to achieve that, we first have to create a dataframe, `subject_df`, where statements with more than one subject are eploded in multiple rows. Then, the subjects for both parties will be fetched and registered in two different dataframes, `democrat_subjects` and `republican_subjects`. We will then only select the 20 most important subject for each party (with `find_top_x_republican` and `find_top_x_democrat`) and make a Venn diagram with these subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN in 'subject'\n",
    "temp1 = liar.dropna(subset=['subject'])\n",
    "\n",
    "# Create a dataframe where statements with more than one subject are eploded in multiple rows\n",
    "subject_list = temp1['subject'].str.split(',').tolist()\n",
    "subject_df = pd.DataFrame(subject_list, index=temp1['statement_id'])\\\n",
    "               .stack()\\\n",
    "               .reset_index()[['statement_id', 0]]\\\n",
    "               .rename(columns={0:'subject'})\\\n",
    "               .merge(temp1.drop('subject', axis=1), on='statement_id')\\\n",
    "               .filter(['statement_id', 'subject', 'label', 'party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_x_republican(col_name, x):\n",
    "    '''\n",
    "    Returns a list of the top x entities with the most occurencies in the column specified by col_name.\n",
    "    :param col_name: str, x: int, sorted: bool\n",
    "    :return: top_ten\n",
    "    '''\n",
    "    # Get the top x\n",
    "    top_x = republican_subjects[['statement_id', col_name]].groupby(col_name)\\\n",
    "                                                           .count()\\\n",
    "                                                           .sort_values('statement_id', ascending=False)\\\n",
    "                                                           .head(x)\\\n",
    "                                                           .index\\\n",
    "                                                           .values\\\n",
    "                                                           .astype('str')\n",
    "    return top_x\n",
    "\n",
    "def find_top_x_democrat(col_name, x):\n",
    "    '''\n",
    "    Returns a list of the top x entities with the most occurencies in the column specified by col_name.\n",
    "    :param col_name: str, x: int, sorted: bool\n",
    "    :return: top_ten\n",
    "    '''\n",
    "    # Get the top x\n",
    "    top_x = democrat_subjects[['statement_id', col_name]].groupby(col_name)\\\n",
    "                                                         .count()\\\n",
    "                                                         .sort_values('statement_id', ascending=False)\\\n",
    "                                                         .head(x)\\\n",
    "                                                         .index\\\n",
    "                                                         .values\\\n",
    "                                                         .astype('str')\n",
    "    return top_x\n",
    "\n",
    "def color_func(word, *args, **kwargs):\n",
    "        '''Set the word color to blue for democrats, to red for republican and to grey for shared subjects'''\n",
    "        if word in [str(elem) for elem in np.arange(20).tolist()]:\n",
    "            return '#000000' # black\n",
    "        elif word in [word for word in democrat_top_subjects if word not in republican_top_subjects]:\n",
    "            return '#0000ff' # blue\n",
    "        elif word in [word for word in republican_top_subjects if word not in democrat_top_subjects]:\n",
    "            return '#ff0000' # red\n",
    "        else:\n",
    "            return '#808080' # gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find subject addressed by democrats\n",
    "democrat_subjects = subject_df.where(liar['party'] == 'democrat')\\\n",
    "                              .dropna()\\\n",
    "                              .filter(['statement_id', 'subject'])\n",
    "\n",
    "democrat_top_subjects_unique = find_top_x_democrat('subject', 20)\n",
    "democrat_filter = democrat_subjects.isin(democrat_top_subjects_unique)\n",
    "democrat_top_subjects = democrat_subjects[democrat_filter['subject']]\\\n",
    "                                         .filter(['subject'])\\\n",
    "                                         .subject.tolist()\n",
    "\n",
    "# Add \"fake subjects\" to improve the look of the diagram by pulling the circles appart\n",
    "# (Note that this have no impact on the conclusion we can draw from the distribution of subjects)\n",
    "democrat_top_subjects = democrat_top_subjects + [str(elem) for elem in np.arange(10).tolist()]\n",
    "\n",
    "# Find subject addressed by republicans\n",
    "republican_subjects = subject_df.where(liar['party'] == 'republican')\\\n",
    "                                .dropna()\\\n",
    "                                .filter(['statement_id', 'subject'])\n",
    "\n",
    "republican_top_subjects_unique = find_top_x_republican('subject', 20)\n",
    "republican_filter = republican_subjects.isin(republican_top_subjects_unique)\n",
    "republican_top_subjects = republican_subjects[republican_filter['subject']]\\\n",
    "                                             .filter(['subject'])\\\n",
    "                                             .subject.tolist()\n",
    "\n",
    "# Add \"fake subjects\" to improve the look of the diagram by pulling the circles appart\n",
    "# (Note that this have no impact on the conclusion we can draw from the distribution of subjects)\n",
    "republican_top_subjects = republican_top_subjects + [str(elem) for elem in np.arange(10,20).tolist()]\n",
    "\n",
    "# Create the Venn diagramm\n",
    "sets = []\n",
    "sets.append(set(democrat_top_subjects))\n",
    "sets.append(set(republican_top_subjects))\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.set_title(\"Subject adressed by democrats and republicans\", fontsize=54, color='#ffffff')\n",
    "venn2_wordcloud(sets,\n",
    "                set_edgecolors=['b', 'r'],\n",
    "                wordcloud_kwargs=dict(color_func=color_func, relative_scaling=0.99, prefer_horizontal=1),\n",
    "                ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
